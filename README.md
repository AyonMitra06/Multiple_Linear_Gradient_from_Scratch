# Multiple_Linear_Gradient_from_Scratch
Multiple Linear Regression from Scratch (Normal Equation)
ğŸ“Œ Overview

This project implements Multiple Linear Regression from scratch using calculus and linear algebra, without using gradient descent or any machine learning libraries for training.

Instead of iterative optimization, the model parameters are derived analytically using the Normal Equation, which provides a closed-form solution for linear regression.

The custom implementation is then validated against an existing library model by comparing predictions using the RÂ² (coefficient of determination) score.
Both models produce identical results, confirming the correctness of the mathematical derivation and implementation.

ğŸ¯ Objectives

Understand the mathematical foundation of linear regression

Derive the model parameters using calculus and matrix algebra

Implement regression from scratch using NumPy only

Compare the custom model with an existing implementation

Verify correctness using RÂ² score

ğŸ§  Mathematical Foundation
1ï¸âƒ£ Model Representation

For multiple linear regression:

ğ‘¦
=
ğ‘‹
ğœƒ
y=XÎ¸

Where:

ğ‘‹
X â†’ feature matrix (with bias term)

ğœƒ
Î¸ â†’ parameter vector

ğ‘¦
y â†’ target values

To account for the intercept, a column of ones is added to the feature matrix.

2ï¸âƒ£ Cost Function (Mean Squared Error)
ğ½
(
ğœƒ
)
=
1
2
ğ‘š
âˆ‘
ğ‘–
=
1
ğ‘š
(
ğ‘‹
ğœƒ
âˆ’
ğ‘¦
)
2
J(Î¸)=
2m
1
	â€‹

i=1
âˆ‘
m
	â€‹

(XÎ¸âˆ’y)
2

In matrix form:

ğ½
(
ğœƒ
)
=
1
2
ğ‘š
(
ğ‘‹
ğœƒ
âˆ’
ğ‘¦
)
ğ‘‡
(
ğ‘‹
ğœƒ
âˆ’
ğ‘¦
)
J(Î¸)=
2m
1
	â€‹

(XÎ¸âˆ’y)
T
(XÎ¸âˆ’y)
3ï¸âƒ£ Derivation Using Calculus

To minimize the cost function, we compute the derivative with respect to 
ğœƒ
Î¸:

âˆ‚
ğ½
(
ğœƒ
)
âˆ‚
ğœƒ
=
1
ğ‘š
ğ‘‹
ğ‘‡
(
ğ‘‹
ğœƒ
âˆ’
ğ‘¦
)
âˆ‚Î¸
âˆ‚J(Î¸)
	â€‹

=
m
1
	â€‹

X
T
(XÎ¸âˆ’y)

Setting the derivative equal to zero:

ğ‘‹
ğ‘‡
ğ‘‹
ğœƒ
=
ğ‘‹
ğ‘‡
ğ‘¦
X
T
XÎ¸=X
T
y
4ï¸âƒ£ Normal Equation (Closed-Form Solution)

Solving for 
ğœƒ
Î¸:

ğœƒ
=
(
ğ‘‹
ğ‘‡
ğ‘‹
)
âˆ’
1
ğ‘‹
ğ‘‡
ğ‘¦
Î¸=(X
T
X)
âˆ’1
X
T
y

This equation gives the optimal parameters directly, without any iteration or learning rate.

ğŸ› ï¸ Implementation Details

Implemented entirely using NumPy

No use of:

Gradient Descent

Scikit-learn training APIs

Matrix operations used:

Transpose

Dot product

Matrix inverse / linear solver

For numerical stability, np.linalg.solve() is preferred over explicit matrix inversion where applicable.

ğŸ“Š Model Evaluation

The model is evaluated using the RÂ² score, which measures how well predictions explain the variance in the target variable.

ğ‘…
2
=
1
âˆ’
âˆ‘
(
ğ‘¦
âˆ’
ğ‘¦
^
)
2
âˆ‘
(
ğ‘¦
âˆ’
ğ‘¦
Ë‰
)
2
R
2
=1âˆ’
âˆ‘(yâˆ’
y
Ë‰
	â€‹

)
2
âˆ‘(yâˆ’
y
^
	â€‹

)
2
	â€‹

ğŸ” Comparison

Custom implementation

Existing library model

Both models produce identical RÂ² scores, confirming:

Correct mathematical derivation

Correct implementation

Equivalent predictive performance

âœ… Results

âœ” Identical RÂ² score for both models

âœ” Exact match in predictions

âœ” Confirms correctness of Normal Equation approach

ğŸ“ Project Structure
â”œâ”€â”€ multiple_linear_regression_from_scratch.ipynb
â”œâ”€â”€ README.md

ğŸš€ Key Takeaways

Linear regression can be solved analytically, not just iteratively

Understanding the math removes the â€œblack boxâ€ nature of ML models

The Normal Equation provides an exact solution when assumptions are met

Custom implementations can match library models when done correctly

ğŸ“š Technologies Used

Python

NumPy

Scikit-learn (for evaluation only)

Jupyter Notebook
